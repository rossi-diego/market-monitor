"""
Machine Learning Forecast Page
------------------------------

This page allows the user to:

1. Select an asset (target variable).
2. Choose which dataset columns will be used as FEATURES.
3. Choose number of LAGS (0 to 10).
4. Select ML model (Ridge, Random Forest, XGBoost).
5. View model performance vs actual (historical backtest).
6. Produce OUT-OF-SAMPLE FORECAST for up to 45 future days.

The page also includes explanations for:
- What are lags?
- How the model learns temporal structure.
- What is multi-step forecasting?
- What MAE, RMSE and RÂ² represent.
- What normalization/standardization is and why we use it.
"""

# ============================================================
# Imports & Setup
# ============================================================
import pandas as pd
import numpy as np
import streamlit as st
from scipy import stats

from sklearn.linear_model import Ridge, Lasso, ElasticNet
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.svm import SVR
from sklearn.neural_network import MLPRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error
from sklearn.preprocessing import StandardScaler

try:
    from xgboost import XGBRegressor
    HAS_XGB = True
except ImportError:
    HAS_XGB = False

try:
    from lightgbm import LGBMRegressor
    HAS_LGBM = True
except ImportError:
    HAS_LGBM = False

from src.data_pipeline import df
from src.utils import apply_theme, date_range_picker, section
import plotly.graph_objects as go

# Theme
apply_theme()

# Page header
st.markdown("# ðŸ”® Machine Learning - PrevisÃ£o de PreÃ§os")
st.markdown("Utilize algoritmos de Machine Learning para prever preÃ§os futuros de commodities com base em dados histÃ³ricos")
st.divider()

# ============================================================
# Base Data
# ============================================================
BASE = df.copy()
BASE["date"] = pd.to_datetime(BASE["date"], errors="coerce")
BASE = BASE.sort_values("date")

# ============================================================
# Explanation Expander
# ============================================================
with st.expander("ðŸ“˜ Como funciona o modelo de Machine Learning?", expanded=False):
    st.markdown("""
    ### ðŸ§  Conceitos Fundamentais

    **1) Lags (Valores HistÃ³ricos)**
    Lags sÃ£o valores passados da prÃ³pria sÃ©rie temporal que estamos prevendo.
    - **Lag 1** = preÃ§o de ontem (t-1)
    - **Lag 2** = preÃ§o de anteontem (t-2)
    - **Lag 5** = preÃ§o de 5 dias atrÃ¡s (t-5)

    Os lags ajudam o modelo a capturar **tendÃªncia, momentum e autocorrelaÃ§Ã£o** da sÃ©rie.

    **2) Features Externas (VariÃ¡veis Explicativas)**
    SÃ£o outras variÃ¡veis que podem influenciar o preÃ§o do ativo alvo:
    - DÃ³lar, commodities relacionadas, prÃªmios, etc.
    - Permitem ao modelo aprender relaÃ§Ãµes causais ou correlaÃ§Ãµes fortes

    **3) Multi-Step Forecasting (PrevisÃ£o Iterativa)**
    Para prever vÃ¡rios dias Ã  frente, o modelo:
    1. PrevÃª o prÃ³ximo dia usando lags reais
    2. Usa essa previsÃ£o como novo lag para prever o dia seguinte
    3. Repete o processo atÃ© completar o horizonte de previsÃ£o

    âš ï¸ **Importante**: Quanto mais dias Ã  frente, maior a incerteza acumulada!

    ### ðŸ“Š MÃ©tricas de AvaliaÃ§Ã£o

    | MÃ©trica | Significado | InterpretaÃ§Ã£o |
    |---------|-------------|---------------|
    | **MAE** | Erro mÃ©dio absoluto | Erro tÃ­pico em unidades do preÃ§o |
    | **RMSE** | Raiz do erro quadrÃ¡tico mÃ©dio | Penaliza erros grandes, mesma unidade do preÃ§o |
    | **RÂ²** | Coeficiente de determinaÃ§Ã£o | % da variÃ¢ncia explicada (0-1). Negativo = pior que mÃ©dia |
    | **MAPE** | Erro percentual mÃ©dio absoluto | Erro em % - mais fÃ¡cil de interpretar |

    ### ðŸŽ¯ Escolhendo o Modelo

    - **Ridge**: RÃ¡pido, simples, bom para relaÃ§Ãµes lineares
    - **Random Forest**: Robusto, captura nÃ£o-linearidades, menos overfitting
    - **XGBoost**: Poderoso, melhor performance, requer mais dados
    """)

st.divider()

# ============================================================
# Configuration Section
# ============================================================
st.markdown("## âš™ï¸ ConfiguraÃ§Ã£o do Modelo")

# Container 1: Target and Features
with st.container(border=True):
    st.markdown("### ðŸŽ¯ Dados de Entrada")

    valid_cols = [c for c in BASE.columns if c not in ["date"] and BASE[c].dtype != "object"]

    col1, col2 = st.columns([1, 2])

    with col1:
        target_col = st.selectbox(
            "Ativo a prever (Target)",
            valid_cols,
            index=0,
            help="VariÃ¡vel que o modelo irÃ¡ prever"
        )

    with col2:
        # Calculate correlation with target to suggest best features
        default_features = []
        if target_col:
            correlations = {}
            for col in valid_cols:
                if col != target_col:
                    try:
                        # Calculate correlation
                        corr_data = BASE[[target_col, col]].dropna()
                        if len(corr_data) > 10:
                            corr = corr_data.corr().iloc[0, 1]
                            correlations[col] = abs(corr)
                    except:
                        pass

            # Get top 5 features by correlation
            if correlations:
                sorted_features = sorted(correlations.items(), key=lambda x: x[1], reverse=True)
                default_features = [feat[0] for feat in sorted_features[:5]]

        # If no correlations found, use first 3
        if not default_features:
            default_features = [c for c in valid_cols if c != target_col][:3]

        feature_cols = st.multiselect(
            "Features (variÃ¡veis explicativas)",
            options=valid_cols,
            default=default_features,
            help="ðŸ’¡ As 5 features mais correlacionadas com o target sÃ£o selecionadas por padrÃ£o"
        )

        # Show correlation info
        if feature_cols and target_col and correlations:
            with st.expander(f"ðŸ“Š CorrelaÃ§Ã£o das Features com {target_col}", expanded=False):
                # Show correlations for selected features
                selected_corrs = [(f, correlations.get(f, 0)) for f in feature_cols if f in correlations]
                selected_corrs.sort(key=lambda x: x[1], reverse=True)

                st.markdown("**Top 5 Features Mais Correlacionadas:**")
                for i, (feat, corr) in enumerate(selected_corrs[:5], 1):
                    corr_strength = "Forte" if corr > 0.7 else "Moderada" if corr > 0.4 else "Fraca"
                    corr_color = "ðŸŸ¢" if corr > 0.7 else "ðŸŸ¡" if corr > 0.4 else "ðŸ”´"
                    st.caption(f"{i}. **{feat}**: {corr:.3f} {corr_color} ({corr_strength})")

    # At least 1 feature or lag must exist
    if len(feature_cols) == 0:
        st.info("ðŸ’¡ Dica: Selecione features ou configure lags abaixo para treinar o modelo.")

# Container 2: Model Configuration
with st.container(border=True):
    st.markdown("### ðŸ¤– ConfiguraÃ§Ã£o do Algoritmo")

    col_model, col_lags, col_horizon = st.columns(3)

    with col_model:
        models_dict = {
            "Ridge Regression": Ridge(alpha=1.0),
            "Lasso Regression": Lasso(alpha=0.01),
            "Elastic Net": ElasticNet(alpha=0.01, l1_ratio=0.5),
            "Random Forest": RandomForestRegressor(n_estimators=500, max_depth=15, random_state=42, n_jobs=-1),
            "Gradient Boosting": GradientBoostingRegressor(n_estimators=200, learning_rate=0.05, max_depth=5, random_state=42),
            "SVR (Support Vector)": SVR(kernel='rbf', C=100, epsilon=0.1),
            "Neural Network (MLP)": MLPRegressor(hidden_layer_sizes=(100, 50), max_iter=1000, learning_rate_init=0.001, random_state=42),
        }
        if HAS_XGB:
            models_dict["XGBoost"] = XGBRegressor(
                n_estimators=600,
                learning_rate=0.05,
                max_depth=6,
                subsample=0.9,
                colsample_bytree=0.9,
                random_state=42,
            )
        if HAS_LGBM:
            models_dict["LightGBM"] = LGBMRegressor(
                n_estimators=600,
                learning_rate=0.05,
                max_depth=6,
                num_leaves=31,
                random_state=42,
                verbose=-1,
            )

        model_label = st.selectbox(
            "Algoritmo",
            list(models_dict.keys()),
            help="Ridge: linear rÃ¡pido | Lasso: com regularizaÃ§Ã£o L1 | RF: nÃ£o-linear robusto | XGB/LGBM: mÃ¡xima performance | SVR: kernel baseado | MLP: redes neurais"
        )
        model = models_dict[model_label]

        # Model description
        model_descriptions = {
            "Ridge Regression": "âœ… RÃ¡pido e interpretÃ¡vel\nâœ… Bom para relaÃ§Ãµes lineares\nâœ… Robusto com multicolinearidade\nâš ï¸ Pode nÃ£o capturar nÃ£o-linearidades",
            "Lasso Regression": "âœ… SeleÃ§Ã£o automÃ¡tica de features\nâœ… Simples e interpretÃ¡vel\nâš ï¸ Menos eficaz com muitas features\nâš ï¸ SensÃ­vel a escala",
            "Elastic Net": "âœ… Combina Ridge + Lasso\nâœ… Bom com muitas features correlacionadas\nâœ… SeleÃ§Ã£o de features\nâš ï¸ Menos preciso que ensemble methods",
            "Random Forest": "âœ… Robusto a outliers\nâœ… Captura nÃ£o-linearidades\nâœ… Menos propenso a overfitting\nâœ… ParalelizÃ¡vel",
            "Gradient Boosting": "âœ… Melhor que RF em muitos casos\nâœ… Captura padrÃµes complexos\nâš ï¸ Risco de overfitting\nâš ï¸ Mais lento para treinar",
            "SVR (Support Vector)": "âœ… Bom com high-dimensional data\nâœ… Kernel flex para nÃ£o-linearidades\nâš ï¸ Requer normalizaÃ§Ã£o\nâš ï¸ Lento com muitos dados",
            "Neural Network (MLP)": "âœ… MÃ¡xima flexibilidade\nâœ… PadrÃµes muito complexos\nâš ï¸ Caixa preta (difÃ­cil interpretar)\nâš ï¸ Requer mais dados",
            "XGBoost": "âœ… Melhor performance geral\nâœ… Captura padrÃµes muito complexos\nâœ… Feature importance confiÃ¡vel\nâš ï¸ Requer mais dados para treinar",
            "LightGBM": "âœ… Mais rÃ¡pido que XGBoost\nâœ… Menor consumo de memÃ³ria\nâœ… Excelente performance\nâš ï¸ Pode overfittar com poucos dados",
        }
        st.caption(model_descriptions.get(model_label, ""))

    with col_lags:
        num_lags = st.slider(
            "NÃºmero de lags",
            min_value=0,
            max_value=3,
            value=0,
            step=1,
            help="Valores histÃ³ricos do target: lag1=ontem, lag2=anteontem, etc."
        )
        if num_lags > 0:
            st.caption(f"âœ“ Usando {num_lags} valores histÃ³ricos")
        else:
            st.caption("âœ“ Apenas features externas")

    with col_horizon:
        horizon = st.slider(
            "Dias Ã  frente",
            min_value=1,
            max_value=45,
            value=30,
            help="Quantidade de dias futuros a prever"
        )
        st.caption(f"ðŸ”® Prevendo {horizon} dias")

# Container 3: Advanced Settings
with st.container(border=True):
    st.markdown("### âš™ï¸ ConfiguraÃ§Ãµes AvanÃ§adas")

    col_norm, col_period = st.columns([1, 2])

    with col_norm:
        normalize = st.checkbox(
            "Normalizar dados (StandardScaler)",
            value=True,
            help="Recomendado: padroniza features para mÃ©dia=0 e std=1"
        )
        if normalize:
            st.caption("âœ“ NormalizaÃ§Ã£o ativada")
        else:
            st.caption("âš ï¸ Usando escala original")

    with col_period:
        start_model_date, end_model_date = date_range_picker(
            BASE["date"],
            state_key="ml_train_range",
            default_days=365 * 3,
        )

    mask_model = (BASE["date"].dt.date >= start_model_date) & (
        BASE["date"].dt.date <= end_model_date
    )
    BASE_RANGE = BASE.loc[mask_model].copy()

    if BASE_RANGE.empty:
        st.error("âŒ Sem dados no perÃ­odo selecionado para treinar o modelo.")
        st.stop()

    st.caption(f"ðŸ“Š Usando {len(BASE_RANGE)} dias de dados histÃ³ricos ({start_model_date} a {end_model_date})")

# Check if we have enough features
if len(feature_cols) == 0 and num_lags == 0:
    st.error("âŒ Configure ao menos uma feature ou um lag para treinar o modelo.")
    st.stop()

st.divider()


# ============================================================
# Prepare dataset (lags + features)
# ============================================================
df_ml = BASE_RANGE[["date", target_col] + feature_cols].copy()

# Ensure date column is datetime type
df_ml["date"] = pd.to_datetime(df_ml["date"], errors="coerce")

# Generate lag columns (on target)
for lag in range(1, num_lags + 1):
    df_ml[f"{target_col}_lag{lag}"] = df_ml[target_col].shift(lag)

# Drop rows with NaN caused by lags or missing features
df_ml = df_ml.dropna().reset_index(drop=True)

if df_ml.empty:
    st.error("Dados insuficientes apÃ³s aplicar lags e filtrar NaNs.")
    st.stop()

# Build X, y
X = df_ml.drop(columns=["date", target_col])
y = df_ml[target_col]

feature_names = X.columns.tolist()

# Train-test split (80/20, temporal)
split = int(len(df_ml) * 0.80)
X_train_raw, X_test_raw = X.iloc[:split].copy(), X.iloc[split:].copy()
y_train_raw, y_test_raw = y.iloc[:split].copy(), y.iloc[split:].copy()
dates_test = df_ml["date"].iloc[split:]

# Scalers (only if normalize=True)
x_scaler = None
y_scaler = None

if normalize:
    x_scaler = StandardScaler()
    X_train = x_scaler.fit_transform(X_train_raw)
    X_test = x_scaler.transform(X_test_raw)

    y_scaler = StandardScaler()
    y_train_scaled = y_scaler.fit_transform(
        y_train_raw.values.reshape(-1, 1)
    ).ravel()

    # Fit on normalized data
    model.fit(X_train, y_train_scaled)

    # Predict on normalized space, then invert back to original scale
    pred_test_scaled = model.predict(X_test)
    pred_test = y_scaler.inverse_transform(
        pred_test_scaled.reshape(-1, 1)
    ).ravel()

    y_test_true = y_test_raw.copy()

else:
    # No normalization: use raw values directly
    X_train = X_train_raw
    X_test = X_test_raw
    y_train = y_train_raw

    model.fit(X_train, y_train)
    pred_test = model.predict(X_test)
    y_test_true = y_test_raw.copy()

# ============================================================
# Show metrics with professional cards
# ============================================================
st.markdown("## ðŸ“Š Performance do Modelo")

with st.container(border=True):
    st.markdown("### ðŸŽ¯ MÃ©tricas de Erro (Conjunto de Teste)")

    # Calculate all metrics
    mae = mean_absolute_error(y_test_true, pred_test)
    rmse = np.sqrt(mean_squared_error(y_test_true, pred_test))
    r2 = r2_score(y_test_true, pred_test)

    # MAPE calculation (handling division by zero properly)
    def safe_mape(y_true, y_pred):
        """Calculate MAPE with proper handling of edge cases."""
        if len(y_true) == 0:
            return None
        
        # Avoid division by zero
        mask = y_true != 0
        if mask.sum() == 0:
            return None
        
        mape_val = np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100
        return mape_val

    mape = safe_mape(y_test_true.values, pred_test)

    # Additional metrics: MASE (Mean Absolute Scaled Error)
    naive_forecast = y_test_true.iloc[:-1].values
    naive_error = np.mean(np.abs(np.diff(y_test_true.values)))
    mase = mae / (naive_error + 1e-8) if naive_error > 0 else np.inf

    # Mean of actual values for context
    y_mean = y_test_true.mean()

    # Display metrics in columns
    col_m1, col_m2, col_m3, col_m4, col_m5 = st.columns(5)

    with col_m1:
        mae_pct = (mae / y_mean * 100) if y_mean != 0 else 0
        mae_color = "ðŸŸ¢" if mae_pct < 3 else "ðŸŸ¡" if mae_pct < 7 else "ðŸ”´"
        st.metric(
            "MAE",
            f"{mae:.2f}",
            f"{mae_color} {mae_pct:.1f}%",
            help="Erro mÃ©dio absoluto em unidades do preÃ§o"
        )

    with col_m2:
        rmse_pct = (rmse / y_mean * 100) if y_mean != 0 else 0
        rmse_color = "ðŸŸ¢" if rmse_pct < 5 else "ðŸŸ¡" if rmse_pct < 10 else "ðŸ”´"
        st.metric(
            "RMSE",
            f"{rmse:.2f}",
            f"{rmse_color} {rmse_pct:.1f}%",
            help="Raiz do erro quadrÃ¡tico mÃ©dio (penaliza grandes erros)"
        )

    with col_m3:
        r2_color = "ðŸŸ¢" if r2 > 0.7 else "ðŸŸ¡" if r2 > 0.3 else "ðŸ”´"
        r2_pct = r2 * 100 if r2 > 0 else 0
        st.metric(
            "RÂ²",
            f"{r2:.3f}",
            f"{r2_color} {r2_pct:.1f}%",
            help="Coeficiente de determinaÃ§Ã£o (% explicada)"
        )

    with col_m4:
        if mape is not None and not np.isinf(mape):
            mape_color = "ðŸŸ¢" if mape < 5 else "ðŸŸ¡" if mape < 10 else "ðŸ”´"
            st.metric(
                "MAPE",
                f"{mape:.1f}%",
                f"{mape_color}",
                help="Erro percentual mÃ©dio absoluto"
            )
        else:
            st.metric("MAPE", "N/A", help="NÃ£o calculÃ¡vel")

    with col_m5:
        if not np.isinf(mase):
            mase_color = "ðŸŸ¢" if mase < 1.0 else "ðŸŸ¡" if mase < 1.5 else "ðŸ”´"
            st.metric(
                "MASE",
                f"{mase:.2f}",
                f"{mase_color}",
                help="Erro escalado pelo naive forecast (< 1.0 = melhor que naive)"
            )
        else:
            st.metric("MASE", "N/A", help="Erro escalado")

    # Interpretation guide
    with st.expander("ðŸ“– Como interpretar as mÃ©tricas"):
        st.markdown(f"""
        ### Contexto
        - **MÃ©dia do target**: {y_mean:.2f}
        - **Desvio padrÃ£o**: {y_test_true.std():.2f}
        - **Samples no teste**: {len(y_test_true)}

        ### InterpretaÃ§Ã£o por MÃ©trica

        **MAE (Mean Absolute Error)**
        - Erro mÃ©dio: {mae:.2f} unidades ({mae_pct:.1f}% da mÃ©dia)
        - {"âœ… Excelente!" if mae_pct < 3 else "âš ï¸ Moderado" if mae_pct < 7 else "âŒ Alto"}
        - Significa que, em mÃ©dia, o modelo erra {mae:.2f} unidades
        - Mais interpretÃ¡vel que RMSE por usar escala original

        **RMSE (Root Mean Squared Error)**
        - RMSE: {rmse:.2f} ({rmse_pct:.1f}% da mÃ©dia)
        - Penaliza outliers e erros grandes mais fortemente
        - {"âœ… Bom desempenho" if rmse_pct < 5 else "âš ï¸ AceitÃ¡vel" if rmse_pct < 10 else "âŒ Revisar modelo"}

        **RÂ² (Coefficient of Determination)**
        - RÂ²: {r2:.3f} ({r2_pct:.1f}% da variÃ¢ncia explicada)
        - {"âœ… Modelo forte" if r2 > 0.7 else "âš ï¸ Modelo moderado" if r2 > 0.3 else "âŒ Modelo fraco" if r2 > 0 else "âŒ Pior que baseline (mÃ©dia)"}
        - {f"Explica {r2_pct:.0f}% da variabilidade dos dados" if r2 > 0 else "NÃ£o consegue melhorar a baseline"}

        **MAPE (Mean Absolute Percentage Error)**
        - {f"MAPE: {mape:.1f}%" if mape and not np.isinf(mape) else "N/A"}
        - {f"âœ… Excelente precisÃ£o" if mape and mape < 5 else f"âš ï¸ PrecisÃ£o moderada" if mape and mape < 10 else f"âŒ Baixa precisÃ£o" if mape else "N/A"}
        - Erro em % do valor real (mais comparÃ¡vel entre diferentes escalas)

        **MASE (Mean Absolute Scaled Error)**
        - {f"MASE: {mase:.2f}" if not np.isinf(mase) else "N/A"}
        - {"âœ… Melhor que naive forecast" if mase < 1.0 else "âš ï¸ Similar ao naive" if mase < 1.5 else "âŒ Pior que naive"}
        - Compara performance com um modelo simples (naive forecast)
        - MASE < 1.0 significa que o modelo Ã© melhor que apenas repetir o Ãºltimo valor

        ### ðŸ§  Qual mÃ©trica usar?
        - **Para interpretaÃ§Ã£o**: Use **MAE** ou **MAPE** (mesma unidade/proporÃ§Ã£o do preÃ§o)
        - **Para otimizaÃ§Ã£o**: Minimize **RMSE** (mais sensÃ­vel a outliers)
        - **Para comparaÃ§Ã£o**: Use **RÂ²** ou **MASE** (independente da escala)
        - **Combinado**: Bom modelo tem MAE baixo + RÂ² alto + MASE < 1.0
        """)

st.divider()


# ============================================================
# Feature Importance (visual + table)
# ============================================================
st.markdown("### ðŸ§  ImportÃ¢ncia das Features")

importance_values = None

# Modelos tipo Ã¡rvore (Random Forest, XGBoost)
if hasattr(model, "feature_importances_"):
    importance_values = model.feature_importances_
    importance_type = "ImportÃ¢ncia (Gini/Gain)"

# Modelos lineares (Ridge) â€“ usamos o valor absoluto dos coeficientes
elif hasattr(model, "coef_"):
    coef = model.coef_
    importance_values = np.abs(np.ravel(coef))
    importance_type = "ImportÃ¢ncia (|Coeficiente|)"

if importance_values is None:
    st.info("â„¹ï¸ O modelo selecionado nÃ£o expÃµe importÃ¢ncia de features de forma direta.")
else:
    fi_df = pd.DataFrame(
        {
            "Feature": feature_names,
            "Importance": importance_values,
        }
    ).sort_values("Importance", ascending=False)

    # Normalize importance to percentage
    fi_df["Importance_Pct"] = (fi_df["Importance"] / fi_df["Importance"].sum()) * 100

    col_chart, col_table = st.columns([2, 1])

    with col_chart:
        # Bar chart of feature importance
        fig_importance = go.Figure()

        fig_importance.add_trace(
            go.Bar(
                x=fi_df["Importance_Pct"],
                y=fi_df["Feature"],
                orientation='h',
                marker=dict(
                    color=fi_df["Importance_Pct"],
                    colorscale='Blues',
                    showscale=False
                ),
                text=fi_df["Importance_Pct"].round(1).astype(str) + '%',
                textposition='outside',
            )
        )

        fig_importance.update_layout(
            title=f"{importance_type}",
            xaxis_title="ImportÃ¢ncia Relativa (%)",
            yaxis_title="Features",
            height=max(300, len(fi_df) * 30),
            showlegend=False,
            yaxis=dict(autorange="reversed"),
        )

        st.plotly_chart(fig_importance, use_container_width=True)

    with col_table:
        st.markdown("**Top Features**")
        st.dataframe(
            fi_df[["Feature", "Importance_Pct"]].head(10).round(2),
            use_container_width=True,
            height=350,
            hide_index=True
        )

    st.caption(
        f"ðŸ’¡ **{importance_type}**: Mostra quais features tÃªm maior impacto nas previsÃµes do modelo. "
        f"Features com maior importÃ¢ncia sÃ£o mais relevantes para prever o target."
    )

st.divider()

# ============================================================
# Residual Analysis (Simplified)
# ============================================================
with st.expander("ðŸ” AnÃ¡lise de ResÃ­duos (DiagnÃ³sticos do Modelo)", expanded=False):
    st.markdown("""
    ### ðŸ“Š O que sÃ£o ResÃ­duos?

    **ResÃ­duos = Valor Real - PrevisÃ£o**
    - **Positivo**: Modelo subestimou (previu menos)
    - **Negativo**: Modelo superestimou (previu mais)
    - **PrÃ³ximo de 0**: PrevisÃ£o precisa
    """)

    # Calculate residuals from test set
    residuals = y_test_true.values - pred_test

    # Simple residual statistics
    col_r1, col_r2, col_r3, col_r4 = st.columns(4)

    with col_r1:
        res_mean = residuals.mean()
        mean_color = "ðŸŸ¢" if abs(res_mean) < 1 else "ðŸŸ¡" if abs(res_mean) < 3 else "ðŸ”´"
        st.metric("ViÃ©s MÃ©dio", f"{res_mean:.2f}", f"{mean_color}",
                 help="Deveria ser ~0. Se positivo, modelo subestima. Se negativo, superestima.")

    with col_r2:
        res_std = residuals.std()
        st.metric("Desvio PadrÃ£o", f"{res_std:.2f}",
                 help="Variabilidade dos erros. Menor Ã© melhor.")

    with col_r3:
        max_error = np.abs(residuals).max()
        st.metric("Maior Erro", f"{max_error:.2f}",
                 help="Pior previsÃ£o observada")

    with col_r4:
        # Percentage of errors within 1 std
        within_1std = (np.abs(residuals) <= res_std).sum() / len(residuals) * 100
        st.metric("Dentro de 1Ïƒ", f"{within_1std:.0f}%",
                 help="% de erros dentro de 1 desvio padrÃ£o (~68% Ã© normal)")

    # Simple residual plot
    fig_residuals = go.Figure()

    fig_residuals.add_trace(
        go.Scatter(
            x=dates_test,
            y=residuals,
            mode="markers",
            name="Erros",
            marker=dict(
                size=6,
                color=residuals,
                colorscale='RdYlGn_r',  # Red for positive errors, green for negative
                showscale=True,
                colorbar=dict(title="Erro"),
                line=dict(width=0.5, color='white')
            ),
            hovertemplate="<b>Data:</b> %{x}<br><b>Erro:</b> %{y:.2f}<extra></extra>"
        )
    )

    # Add zero line
    fig_residuals.add_hline(y=0, line_dash="dash", line_color="gray", line_width=2)

    fig_residuals.update_layout(
        title="Erros do Modelo ao Longo do Tempo",
        xaxis_title="Data",
        yaxis_title="Erro (Real - Previsto)",
        height=400,
        hovermode='x unified',
    )

    st.plotly_chart(fig_residuals, use_container_width=True)

    # Interpretation
    st.markdown("### âœ… Como Interpretar")

    if abs(res_mean) < 1 and within_1std > 60:
        st.success("âœ… **Modelo equilibrado**: Erros pequenos e bem distribuÃ­dos!")
    elif abs(res_mean) > 3:
        st.warning(f"âš ï¸ **ViÃ©s detectado**: Modelo tende a {'subestimar' if res_mean > 0 else 'superestimar'}. Considere ajustar features.")
    elif within_1std < 50:
        st.warning("âš ï¸ **Erros dispersos**: Muitos erros grandes. Considere adicionar mais features ou lags.")
    else:
        st.info("â„¹ï¸ **Modelo aceitÃ¡vel**: Alguns erros, mas razoÃ¡vel para previsÃµes.")

st.divider()


# ============================================================
# Plot historical performance (real vs predicted)
# ============================================================
st.markdown("### ðŸ“ˆ Desempenho HistÃ³rico (Conjunto de Teste)")

fig_hist = go.Figure()

fig_hist.add_trace(
    go.Scatter(
        x=dates_test,
        y=y_test_true,
        mode="lines",
        name="Valor Real",
        line=dict(color="blue", width=2)
    )
)

fig_hist.add_trace(
    go.Scatter(
        x=dates_test,
        y=pred_test,
        mode="lines",
        name="PrevisÃ£o do Modelo",
        line=dict(color="red", width=2, dash="dot")
    )
)

# Add error bands
residuals = y_test_true - pred_test
fig_hist.add_trace(
    go.Scatter(
        x=dates_test,
        y=residuals,
        mode="lines",
        name="Erro (Residual)",
        line=dict(color="gray", width=1),
        yaxis="y2",
        opacity=0.5
    )
)

fig_hist.update_layout(
    title="ComparaÃ§Ã£o: Valores Reais vs PrevisÃµes do Modelo",
    xaxis_title="Data",
    yaxis_title=f"{target_col} (PreÃ§o)",
    yaxis2=dict(
        title="Erro (Residual)",
        overlaying="y",
        side="right",
        showgrid=False
    ),
    hovermode='x unified',
    height=500,
    legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="left", x=0)
)

st.plotly_chart(fig_hist, use_container_width=True)

# Residual statistics
col_res1, col_res2, col_res3 = st.columns(3)
with col_res1:
    st.metric("Erro MÃ©dio", f"{residuals.mean():.2f}", help="ViÃ©s do modelo (deveria ser ~0)")
with col_res2:
    st.metric("Erro Std Dev", f"{residuals.std():.2f}", help="Variabilidade dos erros")
with col_res3:
    max_error = residuals.abs().max()
    st.metric("Maior Erro", f"{max_error:.2f}", help="Maior erro absoluto observado")

st.divider()

# ============================================================
# Multi-step OUT-OF-SAMPLE forecast
# ============================================================
st.markdown(f"### ðŸ”® PrevisÃ£o Futura ({horizon} dias)")

try:
    # Usamos a Ãºltima linha disponÃ­vel (com lags jÃ¡ preenchidos) como ponto de partida
    last_row = df_ml.iloc[-1:].copy()

    # Ensure date is Timestamp type and handle properly
    last_date = pd.to_datetime(df_ml["date"].iloc[-1])

    # Create future dates starting from the day after last date
    # Use timedelta properly to avoid the error
    future_dates = pd.DatetimeIndex([
        last_date + pd.Timedelta(days=i) for i in range(1, horizon + 1)
    ])

    forecast_values = []

    # Linha de features (sem date / target) em escala original
    # Convert DataFrame row to Series for easier manipulation
    current_row = last_row.drop(columns=["date", target_col]).copy().iloc[0]

    for step in range(horizon):
        # 1) Prepara features para previsÃ£o (aplica scaler se necessÃ¡rio)
        # Reshape para 2D para o modelo
        current_row_2d = current_row.values.reshape(1, -1)
        
        if normalize and x_scaler is not None and y_scaler is not None:
            current_x = x_scaler.transform(current_row_2d)
            next_pred_scaled = model.predict(current_x)[0]
            # Volta para a escala original do target
            next_pred = float(y_scaler.inverse_transform(
                np.array([[next_pred_scaled]])
            )[0, 0])
        else:
            next_pred = float(model.predict(current_row_2d)[0])

        forecast_values.append(next_pred)

        # 2) Atualiza apenas os lags do target na linha atual (em escala ORIGINAL)
        if num_lags > 0:
            for i in range(num_lags, 1, -1):
                lag_col = f"{target_col}_lag{i}"
                prev_lag_col = f"{target_col}_lag{i-1}"
                if lag_col in current_row.index and prev_lag_col in current_row.index:
                    current_row[lag_col] = current_row[prev_lag_col]
            current_row[f"{target_col}_lag1"] = next_pred
        # As outras features (externas) permanecem constantes com o Ãºltimo valor conhecido.

    forecast_values = np.array(forecast_values, dtype=float)

    # Calculate adaptive confidence interval (increases with forecast horizon)
    forecast_std = float(residuals.std())
    forecast_steps = np.arange(1, horizon + 1, dtype=float)
    
    # Uncertainty grows with horizon: std * (1 + 0.05 * step)
    uncertainty_multiplier = 1.0 + (0.05 * forecast_steps)
    upper_bound = forecast_values + (1.96 * forecast_std * uncertainty_multiplier)
    lower_bound = forecast_values - (1.96 * forecast_std * uncertainty_multiplier)

    # Combined chart: Historical (last 60 days) + Forecast
    last_60_days = min(60, len(df_ml))
    historical_dates = df_ml["date"].iloc[-last_60_days:]
    historical_values = df_ml[target_col].iloc[-last_60_days:]

    fig_combined = go.Figure()

    # Historical actual values
    fig_combined.add_trace(
        go.Scatter(
            x=historical_dates,
            y=historical_values,
            mode="lines",
            name="HistÃ³rico Real",
            line=dict(color="blue", width=2)
        )
    )

    # Forecast
    fig_combined.add_trace(
        go.Scatter(
            x=future_dates,
            y=forecast_values,
            mode="lines+markers",
            name="PrevisÃ£o",
            line=dict(color="red", width=2, dash="dash"),
            marker=dict(size=5)
        )
    )

    # Confidence interval
    fig_combined.add_trace(
        go.Scatter(
            x=future_dates,
            y=upper_bound,
            mode="lines",
            name="IC Superior (95%)",
            line=dict(color="rgba(255,0,0,0)", width=0),
            showlegend=False
        )
    )

    fig_combined.add_trace(
        go.Scatter(
            x=future_dates,
            y=lower_bound,
            mode="lines",
            name="IC Inferior (95%)",
            line=dict(color="rgba(255,0,0,0)", width=0),
            fill='tonexty',
            fillcolor='rgba(255,0,0,0.2)',
            showlegend=True
        )
    )

    # Add vertical line separating history from forecast
    fig_combined.add_vline(
        x=last_date,
        line_dash="dot",
        line_color="gray",
        annotation_text="InÃ­cio da PrevisÃ£o",
        annotation_position="top"
    )

    fig_combined.update_layout(
        title=f"HistÃ³rico (Ãºltimos {last_60_days} dias) + PrevisÃ£o ({horizon} dias)",
        xaxis_title="Data",
        yaxis_title=target_col,
        hovermode='x unified',
        height=500,
        legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="left", x=0)
    )

    st.plotly_chart(fig_combined, use_container_width=True)

    # Forecast summary
    col_f1, col_f2, col_f3, col_f4 = st.columns(4)
    with col_f1:
        st.metric("Ãšltimo Valor Real", f"{historical_values.iloc[-1]:.2f}", help=f"Ãšltimo valor conhecido ({last_date.date()})")
    with col_f2:
        first_forecast = forecast_values[0]
        change_1d = ((first_forecast - historical_values.iloc[-1]) / historical_values.iloc[-1] * 100)
        st.metric("PrevisÃ£o +1 dia", f"{first_forecast:.2f}", f"{change_1d:+.1f}%")
    with col_f3:
        last_forecast = forecast_values[-1]
        change_horizon = ((last_forecast - historical_values.iloc[-1]) / historical_values.iloc[-1] * 100)
        st.metric(f"PrevisÃ£o +{horizon} dias", f"{last_forecast:.2f}", f"{change_horizon:+.1f}%")
    with col_f4:
        avg_forecast = np.mean(forecast_values)
        st.metric("MÃ©dia Prevista", f"{avg_forecast:.2f}", help=f"MÃ©dia das previsÃµes para os prÃ³ximos {horizon} dias")

    # Forecast uncertainty info
    with st.expander("âš ï¸ Entender a Incerteza das PrevisÃµes"):
        st.markdown(f"""
        ### ðŸ“Š Intervalo de ConfianÃ§a (95%)
        
        A Ã¡rea cinzenta ao redor da previsÃ£o representa a **incerteza** do modelo:
        
        - **Intervalo estreito**: Modelo confiante na previsÃ£o
        - **Intervalo largo**: Maior incerteza (observe com cautela)
        - **Intervalo cresce com o tempo**: Normal! PrevisÃµes distantes sÃ£o menos certeiras
        
        ### ðŸ”¢ Como Ã© calculado?
        
        - Baseado na **variabilidade dos erros histÃ³ricos** (resÃ­duos)
        - Aumenta proporcionalmente com a distÃ¢ncia da previsÃ£o
        - PressupÃµe que padrÃµes futuros serÃ£o similares aos passados
        
        ### âš ï¸ LimitaÃ§Ãµes Importantes:
        
        - **NÃ£o prevÃª events**: Choques de mercado, notÃ­cias, etc nÃ£o sÃ£o previstos
        - **PressupÃµe continuidade**: Assume que padrÃµes histÃ³ricos persistem
        - **Pior longe no futuro**: A incerteza cresce com o horizonte
        - **SensÃ­vel ao perÃ­odo de treinamento**: Diferentes perÃ­odos = diferentes previsÃµes
        
        **Para {horizon} dias:** Intervalo estimado = {forecast_std:.2f} Â± {1.96 * forecast_std:.2f} (base) atÃ© {1.96 * forecast_std * (1 + 0.05*horizon):.2f} (end)
        """)

    st.divider()

except Exception as e:
    st.error(f"âŒ Erro ao gerar previsÃ£o: {str(e)}")
    st.info("ðŸ’¡ Dica: Verifique se hÃ¡ dados suficientes e se as configuraÃ§Ãµes estÃ£o adequadas.")

# ============================================================
# Export functionality
# ============================================================
st.markdown("### ðŸ“¥ Exportar Resultados")

col_exp1, col_exp2 = st.columns(2)

with col_exp1:
    try:
        # Export forecast to CSV
        forecast_df = pd.DataFrame({
            'Data': [d.strftime('%Y-%m-%d') for d in future_dates],
            'PrevisÃ£o': forecast_values,
            'IC_Superior_95': upper_bound,
            'IC_Inferior_95': lower_bound
        })

        csv_forecast = forecast_df.to_csv(index=False).encode('utf-8')
        st.download_button(
            "ðŸ“¥ Baixar PrevisÃµes (CSV)",
            data=csv_forecast,
            file_name=f"previsao_{target_col}_{horizon}dias_{pd.Timestamp.now().strftime('%Y%m%d')}.csv",
            mime="text/csv",
            key="download_forecast_csv",
        )
    except Exception as e:
        st.warning(f"âš ï¸ Erro ao exportar previsÃµes: {str(e)}")

with col_exp2:
    # Export model metrics to CSV
    mape_val = f"{mape:.1f}%" if mape and not np.isinf(mape) else "N/A"
    mase_val = f"{mase:.2f}" if not np.isinf(mase) else "N/A"
    
    metrics_df = pd.DataFrame({
        'MÃ©trica': ['MAE', 'RMSE', 'RÂ²', 'MAPE', 'MASE'],
        'Valor': [mae, rmse, r2, mape_val, mase_val],
        'Contexto': [
            f"{mae_pct:.1f}% da mÃ©dia",
            f"{rmse_pct:.1f}% da mÃ©dia",
            f"{r2_pct:.1f}% explicado" if r2 > 0 else "Negativo",
            mape_val,
            mase_val
        ]
    })

    csv_metrics = metrics_df.to_csv(index=False).encode('utf-8')
    st.download_button(
        "ðŸ“¥ Baixar MÃ©tricas (CSV)",
        data=csv_metrics,
        file_name=f"metricas_modelo_{target_col}_{pd.Timestamp.now().strftime('%Y%m%d')}.csv",
        mime="text/csv",
        key="download_metrics_csv",
    )
